{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\konka\\\\MyRepo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dir = 'C:/Users/konka/MyRepo/Data/train'\n",
    "test_dir  = 'C:/Users/konka/MyRepo/Data/test'\n",
    "nb_train_samples = 302\n",
    "nb_test_samples = 107\n",
    "img_width = 227\n",
    "img_height = 227\n",
    "batch_size=20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Most of these values like nb_filters, nb_rows, nb_columns are mostly trial and error.\n",
    "# There is no definite answer to what is the correct one, just keep trying.\n",
    "# input_shape[0] = 3 because of RGB channels\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(img_width, img_height,3)))\n",
    "\n",
    "# ReLU ensures none of the output value from CONV layer falls below 0\n",
    "# To prevent vanishing gradient problem (failure to learn) from happening\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# For every 2x2 \"pixels\" in the output feature map, pick the highest value,\n",
    "# more like pick the most activated one.\n",
    "# Most importantly, pooling effectively reduces the feature map size by 75%\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Repeat a few times before going into fully connected (dense) layers\n",
    "# You are free to comment them out or add more conv relu pool layers\n",
    "# See what it does to the accuracy\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# We can do as many convolutions and poolings as we like, but it's time to really classify\n",
    "# Again, you can do as many dense layers as you like, use different activations and dropout %\n",
    "# It's all trial and error, this demo code may not be the most optimal after all!\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Final layer, 4 nodes representing each class, softmax activation makes sure every node sums up to 1\n",
    "# That means the output nodes are in % score for each food.\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# The architecture is there. But how to measure how close we are to perfection?\n",
    "# Measure the difference from prediction to ground truth - the loss\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 302 images belonging to 2 classes.\n",
      "Found 107 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_generator = datagen.flow_from_directory(train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "testing_generator = datagen.flow_from_directory(test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "15/15 [==============================] - 219s 15s/step - loss: 0.7569 - acc: 0.5017 - val_loss: 0.7003 - val_acc: 0.3700\n",
      "Epoch 2/25\n",
      "15/15 [==============================] - 205s 14s/step - loss: 0.6945 - acc: 0.5124 - val_loss: 0.6910 - val_acc: 0.6300\n",
      "Epoch 3/25\n",
      "15/15 [==============================] - 202s 13s/step - loss: 0.6929 - acc: 0.5382 - val_loss: 0.6901 - val_acc: 0.6300\n",
      "Epoch 4/25\n",
      "15/15 [==============================] - 199s 13s/step - loss: 0.6937 - acc: 0.4766 - val_loss: 0.6907 - val_acc: 0.6300\n",
      "Epoch 5/25\n",
      "15/15 [==============================] - 203s 14s/step - loss: 0.6931 - acc: 0.5067 - val_loss: 0.6906 - val_acc: 0.6300\n",
      "Epoch 6/25\n",
      "15/15 [==============================] - 209s 14s/step - loss: 0.6935 - acc: 0.4920 - val_loss: 0.6907 - val_acc: 0.6300\n",
      "Epoch 7/25\n",
      "15/15 [==============================] - 217s 14s/step - loss: 0.6937 - acc: 0.4766 - val_loss: 0.6915 - val_acc: 0.6300\n",
      "Epoch 8/25\n",
      "15/15 [==============================] - 224s 15s/step - loss: 0.6930 - acc: 0.5217 - val_loss: 0.6916 - val_acc: 0.6300\n",
      "Epoch 9/25\n",
      "15/15 [==============================] - 215s 14s/step - loss: 0.6934 - acc: 0.4732 - val_loss: 0.6918 - val_acc: 0.6300\n",
      "Epoch 10/25\n",
      "15/15 [==============================] - 211s 14s/step - loss: 0.6935 - acc: 0.4986 - val_loss: 0.6854 - val_acc: 0.6300\n",
      "Epoch 11/25\n",
      "15/15 [==============================] - 208s 14s/step - loss: 0.7027 - acc: 0.4498 - val_loss: 0.6923 - val_acc: 0.6300\n",
      "Epoch 12/25\n",
      "15/15 [==============================] - 188s 13s/step - loss: 0.6932 - acc: 0.5185 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      "15/15 [==============================] - 210s 14s/step - loss: 0.6932 - acc: 0.5033 - val_loss: 0.6939 - val_acc: 0.3700\n",
      "Epoch 14/25\n",
      "15/15 [==============================] - 200s 13s/step - loss: 0.6933 - acc: 0.4749 - val_loss: 0.6942 - val_acc: 0.3700\n",
      "Epoch 15/25\n",
      "15/15 [==============================] - 211s 14s/step - loss: 0.6929 - acc: 0.5415 - val_loss: 0.6943 - val_acc: 0.3700\n",
      "Epoch 16/25\n",
      "15/15 [==============================] - 221s 15s/step - loss: 0.6933 - acc: 0.4769 - val_loss: 0.6947 - val_acc: 0.3700\n",
      "Epoch 17/25\n",
      "15/15 [==============================] - 198s 13s/step - loss: 0.6929 - acc: 0.5214 - val_loss: 0.6948 - val_acc: 0.3700\n",
      "Epoch 18/25\n",
      "15/15 [==============================] - 208s 14s/step - loss: 0.6932 - acc: 0.5067 - val_loss: 0.6951 - val_acc: 0.3700\n",
      "Epoch 19/25\n",
      "15/15 [==============================] - 202s 13s/step - loss: 0.6931 - acc: 0.5117 - val_loss: 0.6951 - val_acc: 0.3700\n",
      "Epoch 20/25\n",
      "15/15 [==============================] - 212s 14s/step - loss: 0.6935 - acc: 0.5017 - val_loss: 0.6946 - val_acc: 0.3700\n",
      "Epoch 21/25\n",
      "15/15 [==============================] - 196s 13s/step - loss: 0.6940 - acc: 0.4225 - val_loss: 0.6934 - val_acc: 0.3700\n",
      "Epoch 22/25\n",
      "15/15 [==============================] - 216s 14s/step - loss: 0.6933 - acc: 0.4866 - val_loss: 0.6926 - val_acc: 0.6300\n",
      "Epoch 23/25\n",
      "15/15 [==============================] - 227s 15s/step - loss: 0.6934 - acc: 0.4667 - val_loss: 0.6925 - val_acc: 0.6300\n",
      "Epoch 24/25\n",
      "15/15 [==============================] - 191s 13s/step - loss: 0.6931 - acc: 0.5174 - val_loss: 0.6922 - val_acc: 0.6300\n",
      "Epoch 25/25\n",
      "15/15 [==============================] - 198s 13s/step - loss: 0.6935 - acc: 0.4508 - val_loss: 0.6924 - val_acc: 0.6300\n"
     ]
    }
   ],
   "source": [
    "#model.fit_generator(\n",
    " #       training_generator,\n",
    "#        samples_per_epoch=nb_train_samples,\n",
    "#        nb_epoch=12,\n",
    "#        validation_data=testing_generator,\n",
    "#        nb_val_samples=nb_test_samples)\n",
    "#model.save_weights('first_try.h5') \n",
    "\n",
    "model.fit_generator(\n",
    "        training_generator,\n",
    "        steps_per_epoch=302 // batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=testing_generator,\n",
    "        validation_steps=107 // batch_size)\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
